{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12c3ba3-08df-49d7-b57a-ad228d4e8222",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exam project by Johanne Bomberg and Michelle David"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673208d9-31c5-4192-bd9b-8e586fe240a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T08:18:31.473902Z",
     "iopub.status.busy": "2022-05-21T08:18:31.473373Z",
     "iopub.status.idle": "2022-05-21T08:18:31.480531Z",
     "shell.execute_reply": "2022-05-21T08:18:31.479066Z",
     "shell.execute_reply.started": "2022-05-21T08:18:31.473851Z"
    }
   },
   "source": [
    "### A Classification project investigating the research hypothesis: Mental state verbs and communication verbs play a significant role in improving the mentalization abilility through reading litterary narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439f7da9-c268-439d-ba95-88e32f677af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:28:50.279773Z",
     "iopub.status.busy": "2022-05-28T11:28:50.279100Z",
     "iopub.status.idle": "2022-05-28T11:28:52.942670Z",
     "shell.execute_reply": "2022-05-28T11:28:52.941244Z",
     "shell.execute_reply.started": "2022-05-28T11:28:50.279718Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f7bb37-df11-4a66-a6c8-2990d8d19b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:28:52.955461Z",
     "iopub.status.busy": "2022-05-28T11:28:52.955247Z",
     "iopub.status.idle": "2022-05-28T11:28:55.578759Z",
     "shell.execute_reply": "2022-05-28T11:28:55.577485Z",
     "shell.execute_reply.started": "2022-05-28T11:28:52.955436Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.21.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e8a334-ee46-4eaf-9d1e-44657cf6378d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:28:55.583097Z",
     "iopub.status.busy": "2022-05-28T11:28:55.582630Z",
     "iopub.status.idle": "2022-05-28T11:28:58.213769Z",
     "shell.execute_reply": "2022-05-28T11:28:58.212484Z",
     "shell.execute_reply.started": "2022-05-28T11:28:55.583056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044d8a75-0ef6-463b-954e-4d30583bb335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:01.093719Z",
     "iopub.status.busy": "2022-05-28T11:29:01.093217Z",
     "iopub.status.idle": "2022-05-28T11:29:01.860734Z",
     "shell.execute_reply": "2022-05-28T11:29:01.859595Z",
     "shell.execute_reply.started": "2022-05-28T11:29:01.093667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from __future__ import print_function\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9753484-00f4-4e9a-995d-b6eb42c5ebcc",
   "metadata": {},
   "source": [
    "## Data-preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2c2472-00d9-4ff6-b56e-20bb8bf25a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:19.110311Z",
     "iopub.status.busy": "2022-05-28T11:29:19.109779Z",
     "iopub.status.idle": "2022-05-28T11:29:19.123848Z",
     "shell.execute_reply": "2022-05-28T11:29:19.123069Z",
     "shell.execute_reply.started": "2022-05-28T11:29:19.110256Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'How the Potato Changed the World by Charles C. Mann.txt'),\n",
       " (2, 'DO13, What makes a woman beautiful?.txt'),\n",
       " (3, 'PM16-Wave.txt'),\n",
       " (4, 'PM16, Fahrenheit 451.txt'),\n",
       " (5, 'DO13, Nonmoral nature.txt'),\n",
       " (6, 'Jane.txt'),\n",
       " (7, 'KC19, The vandercook.txt'),\n",
       " (8, 'KC19, Uncle rock.txt'),\n",
       " (9, 'K16, The tuesday night club.txt'),\n",
       " (10, 'DO13, Beyond the mountain.txt'),\n",
       " (11, 'DO, My oedipus complex.txt'),\n",
       " (12, '(K16, The good lord bird).txt'),\n",
       " (13, 'BV13, The Adventure of the Six Napoleons.txt'),\n",
       " (14, 'K16, Someone.txt'),\n",
       " (15, 'The Runner by Don Delillo.txt'),\n",
       " (16, 'KC13, KC19, Corrie.txt'),\n",
       " (17, 'K16, Command authority, første 23 sider.txt'),\n",
       " (18, 'J13, Saffron dreams, 9 første sider.txt'),\n",
       " (19, 'BV13, Blindness, Kapitel 1.txt'),\n",
       " (20, 'BB, The CIA’s Most Highly-Trained Spies Weren’t Even Human .txt'),\n",
       " (21, '(KC13, Salvage the bones).txt'),\n",
       " (22, 'KC13, Sins of the mother.txt'),\n",
       " (23, 'KC13, The chamelion.txt'),\n",
       " (24, 'KC13, Blind date.txt'),\n",
       " (25, '(KC13, The round house).txt'),\n",
       " (26, 'K16, Labor day, første 25 sider ca..txt'),\n",
       " (27, 'DO13, Science and literature.txt'),\n",
       " (28, 'K16, Be with me, første 13 sider.txt'),\n",
       " (29, 'BB, Puppy.txt'),\n",
       " (30, 'DO13, Dreams of the death of beloved persons.txt'),\n",
       " (31, 'DO13, The echo.txt'),\n",
       " (32, 'KC13, Leak.txt'),\n",
       " (33, 'KC19, Space Jockey.txt'),\n",
       " (34, 'K16, The end of the point.txt'),\n",
       " (35, 'KC13, Too many have lived.txt'),\n",
       " (36, 'KC13, Bamboo steps up.txt'),\n",
       " (37, '(J13, Automobile).txt'),\n",
       " (38, 'K16, Pacific, første 21 sider.txt'),\n",
       " (39, 'PM16-Tenth-of-december.txt'),\n",
       " (40, 'KC13, Gone girl.txt'),\n",
       " (41, 'DO13, Killing for sport.txt'),\n",
       " (42, 'BB, The story of the most common bird.txt'),\n",
       " (43, 'DO13, Night club.txt')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates variable \"path\" for all files in current folder:\n",
    "path = os.getcwd()\n",
    "#Creates a list of files from the path that ends with \".txt\":\n",
    "list_of_texts = [file for file in os.listdir(path) if file.endswith(\".txt\")]\n",
    "#Enumerates the list of files beginning at 1:\n",
    "list_of_texts_enumerated = list(enumerate(list_of_texts, start = 1))\n",
    "list_of_texts_enumerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ef4fa4-d9b2-4cb7-9449-24127d4c3682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:21.383909Z",
     "iopub.status.busy": "2022-05-28T11:29:21.383361Z",
     "iopub.status.idle": "2022-05-28T11:29:21.401524Z",
     "shell.execute_reply": "2022-05-28T11:29:21.400337Z",
     "shell.execute_reply.started": "2022-05-28T11:29:21.383855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, False),\n",
       " (2, True),\n",
       " (3, False),\n",
       " (4, False),\n",
       " (5, True),\n",
       " (6, False),\n",
       " (7, True),\n",
       " (8, True),\n",
       " (9, False),\n",
       " (10, True),\n",
       " (11, True),\n",
       " (12, True),\n",
       " (13, True),\n",
       " (14, True),\n",
       " (15, True),\n",
       " (16, True),\n",
       " (17, False),\n",
       " (18, True),\n",
       " (19, True),\n",
       " (20, False),\n",
       " (21, True),\n",
       " (22, False),\n",
       " (23, True),\n",
       " (24, True),\n",
       " (25, True),\n",
       " (26, False),\n",
       " (27, True),\n",
       " (28, False),\n",
       " (29, True),\n",
       " (30, True),\n",
       " (31, True),\n",
       " (32, True),\n",
       " (33, False),\n",
       " (34, True),\n",
       " (35, False),\n",
       " (36, False),\n",
       " (37, False),\n",
       " (38, True),\n",
       " (39, True),\n",
       " (40, False),\n",
       " (41, True),\n",
       " (42, False),\n",
       " (43, False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A list of the mentalization effects where True is for texts with an effect and False is for texts with no effect. Enumerated to place the correct tag (True/False) according to the correct text \n",
    "List_of_mentalization_effect = [False, True, False, False, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, True, False, True, False, True, True, True, True, False, True, False, False, False, True, True, False, True, False, False ]\n",
    "List_of_mentalization_effect_enumerated = list(enumerate(List_of_mentalization_effect, start = 1))\n",
    "List_of_mentalization_effect_enumerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8514d32c-b8fe-4290-a8ae-40e0fbdb59c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:37.520876Z",
     "iopub.status.busy": "2022-05-28T11:29:37.520435Z",
     "iopub.status.idle": "2022-05-28T11:29:37.534399Z",
     "shell.execute_reply": "2022-05-28T11:29:37.533284Z",
     "shell.execute_reply.started": "2022-05-28T11:29:37.520831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Text input  Mentalization effect\n",
      "0   How the Potato Changed the World by Charles C....                 False\n",
      "1             DO13, What makes a woman beautiful?.txt                  True\n",
      "2                                       PM16-Wave.txt                 False\n",
      "3                            PM16, Fahrenheit 451.txt                 False\n",
      "4                           DO13, Nonmoral nature.txt                  True\n",
      "5                                            Jane.txt                 False\n",
      "6                            KC19, The vandercook.txt                  True\n",
      "7                                KC19, Uncle rock.txt                  True\n",
      "8                     K16, The tuesday night club.txt                 False\n",
      "9                       DO13, Beyond the mountain.txt                  True\n",
      "10                         DO, My oedipus complex.txt                  True\n",
      "11                      (K16, The good lord bird).txt                  True\n",
      "12       BV13, The Adventure of the Six Napoleons.txt                  True\n",
      "13                                   K16, Someone.txt                  True\n",
      "14                      The Runner by Don Delillo.txt                  True\n",
      "15                             KC13, KC19, Corrie.txt                  True\n",
      "16        K16, Command authority, første 23 sider.txt                 False\n",
      "17            J13, Saffron dreams, 9 første sider.txt                  True\n",
      "18                     BV13, Blindness, Kapitel 1.txt                  True\n",
      "19  BB, The CIA’s Most Highly-Trained Spies Weren’...                 False\n",
      "20                      (KC13, Salvage the bones).txt                  True\n",
      "21                       KC13, Sins of the mother.txt                 False\n",
      "22                            KC13, The chamelion.txt                  True\n",
      "23                               KC13, Blind date.txt                  True\n",
      "24                        (KC13, The round house).txt                  True\n",
      "25            K16, Labor day, første 25 sider ca..txt                 False\n",
      "26                   DO13, Science and literature.txt                  True\n",
      "27               K16, Be with me, første 13 sider.txt                 False\n",
      "28                                      BB, Puppy.txt                  True\n",
      "29   DO13, Dreams of the death of beloved persons.txt                  True\n",
      "30                                 DO13, The echo.txt                  True\n",
      "31                                     KC13, Leak.txt                  True\n",
      "32                             KC19, Space Jockey.txt                 False\n",
      "33                      K16, The end of the point.txt                  True\n",
      "34                      KC13, Too many have lived.txt                 False\n",
      "35                          KC13, Bamboo steps up.txt                 False\n",
      "36                              (J13, Automobile).txt                 False\n",
      "37                  K16, Pacific, første 21 sider.txt                  True\n",
      "38                         PM16-Tenth-of-december.txt                  True\n",
      "39                                KC13, Gone girl.txt                 False\n",
      "40                        DO13, Killing for sport.txt                  True\n",
      "41          BB, The story of the most common bird.txt                 False\n",
      "42                               DO13, Night club.txt                 False\n"
     ]
    }
   ],
   "source": [
    "#Combines the two lists into a dataframe with texts in first coulmn and their effect in second column:\n",
    "#For an overview of the data\n",
    "dataset = {\"Text input\": list_of_texts, \"Mentalization effect\": List_of_mentalization_effect}\n",
    "Dataframe_of_text_corpora_woth_tags = pd.DataFrame(dataset)\n",
    "print (Dataframe_of_text_corpora_woth_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08d5706-d7de-4358-a4b9-14e11f29b38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:38.920138Z",
     "iopub.status.busy": "2022-05-28T11:29:38.919608Z",
     "iopub.status.idle": "2022-05-28T11:29:38.933760Z",
     "shell.execute_reply": "2022-05-28T11:29:38.932760Z",
     "shell.execute_reply.started": "2022-05-28T11:29:38.920084Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How the Potato Changed the World by Charles C. Mann.txt', False),\n",
       " ('DO13, What makes a woman beautiful?.txt', True),\n",
       " ('PM16-Wave.txt', False),\n",
       " ('PM16, Fahrenheit 451.txt', False),\n",
       " ('DO13, Nonmoral nature.txt', True),\n",
       " ('Jane.txt', False),\n",
       " ('KC19, The vandercook.txt', True),\n",
       " ('KC19, Uncle rock.txt', True),\n",
       " ('K16, The tuesday night club.txt', False),\n",
       " ('DO13, Beyond the mountain.txt', True),\n",
       " ('DO, My oedipus complex.txt', True),\n",
       " ('(K16, The good lord bird).txt', True),\n",
       " ('BV13, The Adventure of the Six Napoleons.txt', True),\n",
       " ('K16, Someone.txt', True),\n",
       " ('The Runner by Don Delillo.txt', True),\n",
       " ('KC13, KC19, Corrie.txt', True),\n",
       " ('K16, Command authority, første 23 sider.txt', False),\n",
       " ('J13, Saffron dreams, 9 første sider.txt', True),\n",
       " ('BV13, Blindness, Kapitel 1.txt', True),\n",
       " ('BB, The CIA’s Most Highly-Trained Spies Weren’t Even Human .txt', False),\n",
       " ('(KC13, Salvage the bones).txt', True),\n",
       " ('KC13, Sins of the mother.txt', False),\n",
       " ('KC13, The chamelion.txt', True),\n",
       " ('KC13, Blind date.txt', True),\n",
       " ('(KC13, The round house).txt', True),\n",
       " ('K16, Labor day, første 25 sider ca..txt', False),\n",
       " ('DO13, Science and literature.txt', True),\n",
       " ('K16, Be with me, første 13 sider.txt', False),\n",
       " ('BB, Puppy.txt', True),\n",
       " ('DO13, Dreams of the death of beloved persons.txt', True),\n",
       " ('DO13, The echo.txt', True),\n",
       " ('KC13, Leak.txt', True),\n",
       " ('KC19, Space Jockey.txt', False),\n",
       " ('K16, The end of the point.txt', True),\n",
       " ('KC13, Too many have lived.txt', False),\n",
       " ('KC13, Bamboo steps up.txt', False),\n",
       " ('(J13, Automobile).txt', False),\n",
       " ('K16, Pacific, første 21 sider.txt', True),\n",
       " ('PM16-Tenth-of-december.txt', True),\n",
       " ('KC13, Gone girl.txt', False),\n",
       " ('DO13, Killing for sport.txt', True),\n",
       " ('BB, The story of the most common bird.txt', False),\n",
       " ('DO13, Night club.txt', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combines the list of text with the list of mentalization-effects into a list of tuples\n",
    "text_corpora_with_tags = zip(list_of_texts, List_of_mentalization_effect)\n",
    "text_corpora_with_tags = list(text_corpora_with_tags)\n",
    "text_corpora_with_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223acb4-746d-400f-9114-32f6f5385062",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text-normalization and feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9401524c-6ba9-40cd-a367-bab274507bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:46.943689Z",
     "iopub.status.busy": "2022-05-28T11:29:46.943166Z",
     "iopub.status.idle": "2022-05-28T11:29:46.953337Z",
     "shell.execute_reply": "2022-05-28T11:29:46.951917Z",
     "shell.execute_reply.started": "2022-05-28T11:29:46.943636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizer_function(file_name):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    #Opens the file while it is worked on as a readable format:\n",
    "    with open(file_name) as f:\n",
    "        text=f.read()\n",
    "    #Turns the text into a string:\n",
    "    text = str(text)\n",
    "    #Tokenizes the text at everythin other than a letter:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #Lowers all characters and puts then in a list:\n",
    "    tokenized_text = [word.lower() for word in tokens]\n",
    "    #Returns list of tokens \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01550ec8-43fd-476d-99c0-5e579099baca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:52.603407Z",
     "iopub.status.busy": "2022-05-28T11:29:52.602568Z",
     "iopub.status.idle": "2022-05-28T11:29:52.615471Z",
     "shell.execute_reply": "2022-05-28T11:29:52.614774Z",
     "shell.execute_reply.started": "2022-05-28T11:29:52.603340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    mental state verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens; the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB-tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    #Creates a list of mental state verbs:    \n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list: \n",
    "    Frecuency_dictionary_mental_state_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_mental_state_verbs)\n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_mental_state_verbs\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e611fa7c-e31f-4027-bbd8-3286d03c496e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:29:59.815753Z",
     "iopub.status.busy": "2022-05-28T11:29:59.815219Z",
     "iopub.status.idle": "2022-05-28T11:30:01.103657Z",
     "shell.execute_reply": "2022-05-28T11:30:01.102894Z",
     "shell.execute_reply.started": "2022-05-28T11:29:59.815699Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'know': 10, 'think': 5, 'see': 5, 'want': 5, 'get': 4, 'like': 3, 'imagine': 3, 'remember': 2, 'feel': 1, 'find': 1, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example of the function:\n",
    "mentalization_features('KC13, Blind date.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af55c344-3fd5-44e6-8bf1-0e80b7f0c2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:06.169900Z",
     "iopub.status.busy": "2022-05-28T11:30:06.169377Z",
     "iopub.status.idle": "2022-05-28T11:30:17.016171Z",
     "shell.execute_reply": "2022-05-28T11:30:17.014574Z",
     "shell.execute_reply.started": "2022-05-28T11:30:06.169845Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66382e0b-5ead-4f20-b2a8-17f1a587e7d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:18.141373Z",
     "iopub.status.busy": "2022-05-28T11:30:18.140845Z",
     "iopub.status.idle": "2022-05-28T11:30:18.205911Z",
     "shell.execute_reply": "2022-05-28T11:30:18.205045Z",
     "shell.execute_reply.started": "2022-05-28T11:30:18.141317Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An example of the classification:\n",
    "classifier.classify(mentalization_features('KC13, Blind date.txt'))\n",
    "#Classifyes the example as True which happens to be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "695db130-25b8-4fd6-a73d-e1f5d9467482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:20.823768Z",
     "iopub.status.busy": "2022-05-28T11:30:20.823241Z",
     "iopub.status.idle": "2022-05-28T11:30:20.837172Z",
     "shell.execute_reply": "2022-05-28T11:30:20.836227Z",
     "shell.execute_reply.started": "2022-05-28T11:30:20.823712Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7858182e-7897-415a-9092-32c9decabdcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:28.301384Z",
     "iopub.status.busy": "2022-05-28T11:30:28.300845Z",
     "iopub.status.idle": "2022-05-28T11:30:28.311647Z",
     "shell.execute_reply": "2022-05-28T11:30:28.310228Z",
     "shell.execute_reply.started": "2022-05-28T11:30:28.301329Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  expect = 1               False : True   =      4.5 : 1.0\n",
      "                    find = 5               False : True   =      4.0 : 1.0\n",
      "                    seek = 1               False : True   =      3.3 : 1.0\n",
      "                 believe = None            False : True   =      2.9 : 1.0\n",
      "                  expect = None             True : False  =      2.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Defines the 5 most informative features:\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4791769-a5e0-4333-8ee3-7aeff41c1fec",
   "metadata": {},
   "source": [
    "## Adjusting the estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a17a24f-f28d-4370-8ef2-47ec4c2dabd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:38.930901Z",
     "iopub.status.busy": "2022-05-28T11:30:38.930382Z",
     "iopub.status.idle": "2022-05-28T11:30:38.948134Z",
     "shell.execute_reply": "2022-05-28T11:30:38.947038Z",
     "shell.execute_reply.started": "2022-05-28T11:30:38.930847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_1(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    mental state verbs, subtracting the percentage of mental state verbs over all verbs.\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    #Creates a list of mental state verbs:    \n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list: \n",
    "    Frecuency_dictionary_mental_state_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_mental_state_verbs)\n",
    "    #Goes trough the keys and values of the dictionary to make the keys into the words and the values into the frequency in percentage.\n",
    "    for (word, frequency) in Frecuency_dictionary_mental_state_verbs.items():\n",
    "        Frecuency_dictionary_mental_state_verbs[word] = frequency/len(verbs)\n",
    "    #Returns dictionary\n",
    "    return Frecuency_dictionary_mental_state_verbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a5ead2-b543-4d49-9aed-c2c54b4ae47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:40.097209Z",
     "iopub.status.busy": "2022-05-28T11:30:40.096692Z",
     "iopub.status.idle": "2022-05-28T11:30:40.223472Z",
     "shell.execute_reply": "2022-05-28T11:30:40.222656Z",
     "shell.execute_reply.started": "2022-05-28T11:30:40.097156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'know': 0.02663115845539281, 'get': 0.007989347536617843, 'want': 0.006657789613848202, 'think': 0.005326231691078562, 'expect': 0.002663115845539281, 'dream': 0.002663115845539281, 'seek': 0.002663115845539281, 'plan': 0.002663115845539281, 'notice': 0.0013315579227696406, 'see': 0.0013315579227696406, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An Example of the function:\n",
    "mentalization_features_1('KC13, Sins of the mother.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8239a923-af6b-4812-9ee3-623702b35864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:41.524270Z",
     "iopub.status.busy": "2022-05-28T11:30:41.523740Z",
     "iopub.status.idle": "2022-05-28T11:30:52.442963Z",
     "shell.execute_reply": "2022-05-28T11:30:52.441869Z",
     "shell.execute_reply.started": "2022-05-28T11:30:41.524217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_1(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_1 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71fa80e-2e95-461c-a4c2-0c6b3a880613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:30:55.900113Z",
     "iopub.status.busy": "2022-05-28T11:30:55.899569Z",
     "iopub.status.idle": "2022-05-28T11:30:55.915042Z",
     "shell.execute_reply": "2022-05-28T11:30:55.913696Z",
     "shell.execute_reply.started": "2022-05-28T11:30:55.900058Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_1, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb6d420-5e27-4be3-8eb1-dce3aff187de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:01.328869Z",
     "iopub.status.busy": "2022-05-28T11:31:01.328352Z",
     "iopub.status.idle": "2022-05-28T11:31:01.338662Z",
     "shell.execute_reply": "2022-05-28T11:31:01.337757Z",
     "shell.execute_reply.started": "2022-05-28T11:31:01.328814Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  figure = None             True : False  =      2.4 : 1.0\n",
      "                  decide = None             True : False  =      2.1 : 1.0\n",
      "                    hope = None             True : False  =      1.9 : 1.0\n",
      "                  forget = None            False : True   =      1.8 : 1.0\n",
      "                    plan = None             True : False  =      1.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_1.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baeb1e41-4367-4e8e-a407-a94b8df76f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:04.715138Z",
     "iopub.status.busy": "2022-05-28T11:31:04.714604Z",
     "iopub.status.idle": "2022-05-28T11:31:04.730737Z",
     "shell.execute_reply": "2022-05-28T11:31:04.730025Z",
     "shell.execute_reply.started": "2022-05-28T11:31:04.715085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_2(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    communications verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    # Creates a list of communication verbs:\n",
    "    list_of_communication_verbs = [\"render\", \"imply\", \"talk\", \"affirm\", \"utter\", \"state\", \"declare\", \"voice\", \"express\", \"pronounce\", \"articulate\", \"enunciate\", \"voclize\", \"verbalize\", \"suggest\", \"indicate\", \"convey\", \"chat\", \"gossip\", \"prattle\", \"gab\", \"blather\", \"communicate\", \"negotiate\", \"parley\", \"confabulate\", \"prate\", \"confess\", \"mouth\", \"profess\", \"answer\", \"remark\", \"speak\", \"say\", \"tell\", \"whisper\", \"murmur\", \"mumble\", \"mutter\", \"yell\", \"shout\", \"cry\", \"call\", \"roar\", \"howl\", \"bellow\", \"bawl\", \"cheer\", \"yelp\", \"squawk\", \"shriek\", \"scream\", \"screech\", \"squeal\", \"squall\", \"whoop\", \"holler\", \"clamour\", \"caterwaul\", \"yawp\", \"vociferate\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental ste list: \n",
    "    Frecuency_dictionary_communication_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_communication_verbs)\n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_communication_verbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383d6a33-105e-4b41-85c3-b5940369ae0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:06.926193Z",
     "iopub.status.busy": "2022-05-28T11:31:06.925650Z",
     "iopub.status.idle": "2022-05-28T11:31:17.806287Z",
     "shell.execute_reply": "2022-05-28T11:31:17.805440Z",
     "shell.execute_reply.started": "2022-05-28T11:31:06.926138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_2(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_2 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75739131-928e-4636-91b1-a12369871f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:18.871716Z",
     "iopub.status.busy": "2022-05-28T11:31:18.871208Z",
     "iopub.status.idle": "2022-05-28T11:31:18.883794Z",
     "shell.execute_reply": "2022-05-28T11:31:18.882870Z",
     "shell.execute_reply.started": "2022-05-28T11:31:18.871663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_2, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5f11237-c2b7-4e32-ae47-7e36dd2f9d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:30.397907Z",
     "iopub.status.busy": "2022-05-28T11:31:30.397391Z",
     "iopub.status.idle": "2022-05-28T11:31:30.406887Z",
     "shell.execute_reply": "2022-05-28T11:31:30.405859Z",
     "shell.execute_reply.started": "2022-05-28T11:31:30.397852Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  squeal = 1               False : True   =      3.5 : 1.0\n",
      "                    yell = 1               False : True   =      2.4 : 1.0\n",
      "                     cry = 1               False : True   =      2.3 : 1.0\n",
      "                    talk = 2               False : True   =      2.2 : 1.0\n",
      "                  squeal = None             True : False  =      2.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_2.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bdee186-d69f-42ed-aa62-5956dad4a750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:37.319577Z",
     "iopub.status.busy": "2022-05-28T11:31:37.319063Z",
     "iopub.status.idle": "2022-05-28T11:31:37.336104Z",
     "shell.execute_reply": "2022-05-28T11:31:37.335358Z",
     "shell.execute_reply.started": "2022-05-28T11:31:37.319524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_3(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    communications verbs and mental state verbs\n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    # Creates a list of communication verbs:\n",
    "    list_of_communication_verbs = [\"render\", \"imply\", \"talk\", \"affirm\", \"utter\", \"state\", \"declare\", \"voice\", \"express\", \"pronounce\", \"articulate\", \"enunciate\", \"voclize\", \"verbalize\", \"suggest\", \"indicate\", \"convey\", \"chat\", \"gossip\", \"prattle\", \"gab\", \"blather\", \"communicate\", \"negotiate\", \"parley\", \"confabulate\", \"prate\", \"confess\", \"mouth\", \"profess\", \"answer\", \"remark\", \"speak\", \"say\", \"tell\", \"whisper\", \"murmur\", \"mumble\", \"mutter\", \"yell\", \"shout\", \"cry\", \"call\", \"roar\", \"howl\", \"bellow\", \"bawl\", \"cheer\", \"yelp\", \"squawk\", \"shriek\", \"scream\", \"screech\", \"squeal\", \"squall\", \"whoop\", \"holler\", \"clamour\", \"caterwaul\", \"yawp\", \"vociferate\"]\n",
    "    # Creates a list of mental_state_verbs:\n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list or in the communication_verbs lsit: \n",
    "    Frecuency_dictionary_communication_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_communication_verbs + list_of_mental_state_verbs)\n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_communication_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ac592d-3622-41ec-af42-48251423140d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:31:49.027542Z",
     "iopub.status.busy": "2022-05-28T11:31:49.027029Z",
     "iopub.status.idle": "2022-05-28T11:32:00.036336Z",
     "shell.execute_reply": "2022-05-28T11:32:00.035670Z",
     "shell.execute_reply.started": "2022-05-28T11:31:49.027488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_3(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_3 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "624e097f-a5b8-4218-ab04-972bcd0db9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:32:13.847549Z",
     "iopub.status.busy": "2022-05-28T11:32:13.847039Z",
     "iopub.status.idle": "2022-05-28T11:32:13.860318Z",
     "shell.execute_reply": "2022-05-28T11:32:13.859522Z",
     "shell.execute_reply.started": "2022-05-28T11:32:13.847494Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_3, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b044cc-bffc-468f-b83c-7b96059150c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:32:47.515661Z",
     "iopub.status.busy": "2022-05-28T11:32:47.515137Z",
     "iopub.status.idle": "2022-05-28T11:32:47.525305Z",
     "shell.execute_reply": "2022-05-28T11:32:47.524498Z",
     "shell.execute_reply.started": "2022-05-28T11:32:47.515607Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 believe = None            False : True   =      4.0 : 1.0\n",
      "              understand = None            False : True   =      3.4 : 1.0\n",
      "                  squeal = 1               False : True   =      2.8 : 1.0\n",
      "                 suppose = 1               False : True   =      2.6 : 1.0\n",
      "                remember = 1               False : True   =      2.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_3.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8082b751-6cba-4737-950b-6bf823694bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:32:53.420662Z",
     "iopub.status.busy": "2022-05-28T11:32:53.420144Z",
     "iopub.status.idle": "2022-05-28T11:32:53.437397Z",
     "shell.execute_reply": "2022-05-28T11:32:53.436698Z",
     "shell.execute_reply.started": "2022-05-28T11:32:53.420607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_4(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    the total amount of mental state verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    #Creates a list of mental state verbs:    \n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list: \n",
    "    Frecuency_dictionary_mental_state_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_mental_state_verbs)\n",
    "    #Creates a variable with frequencies:\n",
    "    values = Frecuency_dictionary_mental_state_verbs.values()\n",
    "    #Sum the values:\n",
    "    total = sum(values)\n",
    "    #Creates new dictionary:\n",
    "    Frecuency_dictionary_mental_state_verbs_total = {\"mental state verbs\": total} \n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_mental_state_verbs_total\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5689dce2-116a-465c-a50d-896810b79de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:32:55.560274Z",
     "iopub.status.busy": "2022-05-28T11:32:55.559746Z",
     "iopub.status.idle": "2022-05-28T11:33:06.513361Z",
     "shell.execute_reply": "2022-05-28T11:33:06.512557Z",
     "shell.execute_reply.started": "2022-05-28T11:32:55.560223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_4(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_4 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "facb30cf-5300-4ab6-8d5e-2b3a077ff982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:33:08.253791Z",
     "iopub.status.busy": "2022-05-28T11:33:08.253127Z",
     "iopub.status.idle": "2022-05-28T11:33:08.263815Z",
     "shell.execute_reply": "2022-05-28T11:33:08.262430Z",
     "shell.execute_reply.started": "2022-05-28T11:33:08.253734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_4, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2db4569-753b-4a5d-b20c-15325ed9d6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:33:23.977213Z",
     "iopub.status.busy": "2022-05-28T11:33:23.976682Z",
     "iopub.status.idle": "2022-05-28T11:33:23.992509Z",
     "shell.execute_reply": "2022-05-28T11:33:23.991504Z",
     "shell.execute_reply.started": "2022-05-28T11:33:23.977159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_5(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    the total amount of communication verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    #Creates a list of communication verbs:    \n",
    "    list_of_communication_verbs = [\"render\", \"imply\", \"talk\", \"affirm\", \"utter\", \"state\", \"declare\", \"voice\", \"express\", \"pronounce\", \"articulate\", \"enunciate\", \"voclize\", \"verbalize\", \"suggest\", \"indicate\", \"convey\", \"chat\", \"gossip\", \"prattle\", \"gab\", \"blather\", \"communicate\", \"negotiate\", \"parley\", \"confabulate\", \"prate\", \"confess\", \"mouth\", \"profess\", \"answer\", \"remark\", \"speak\", \"say\", \"tell\", \"whisper\", \"murmur\", \"mumble\", \"mutter\", \"yell\", \"shout\", \"cry\", \"call\", \"roar\", \"howl\", \"bellow\", \"bawl\", \"cheer\", \"yelp\", \"squawk\", \"shriek\", \"scream\", \"screech\", \"squeal\", \"squall\", \"whoop\", \"holler\", \"clamour\", \"caterwaul\", \"yawp\", \"vociferate\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental ste list: \n",
    "    Frecuency_dictionary_communication_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_communication_verbs)\n",
    "    #Creates a variable with frequencies:\n",
    "    values = Frecuency_dictionary_communication_verbs.values()\n",
    "    #Sum the values:\n",
    "    total = sum(values)\n",
    "    #Creates new dictionary:\n",
    "    Frecuency_dictionary_communication_verbs_total = {\"communication verbs\": total} \n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_communication_verbs_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aecdb04d-fb94-4ade-8e08-99705eec0696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:33:28.125868Z",
     "iopub.status.busy": "2022-05-28T11:33:28.125355Z",
     "iopub.status.idle": "2022-05-28T11:33:38.994035Z",
     "shell.execute_reply": "2022-05-28T11:33:38.992930Z",
     "shell.execute_reply.started": "2022-05-28T11:33:28.125814Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_5(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_5 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4cb09b2-99d1-460f-8144-9fd4f6617e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:33:39.817656Z",
     "iopub.status.busy": "2022-05-28T11:33:39.817011Z",
     "iopub.status.idle": "2022-05-28T11:33:39.826135Z",
     "shell.execute_reply": "2022-05-28T11:33:39.825315Z",
     "shell.execute_reply.started": "2022-05-28T11:33:39.817600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_5, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47757a36-9ab9-4f8a-8609-2ab6ac79b9eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:01.126277Z",
     "iopub.status.busy": "2022-05-28T11:34:01.125737Z",
     "iopub.status.idle": "2022-05-28T11:34:01.144408Z",
     "shell.execute_reply": "2022-05-28T11:34:01.143704Z",
     "shell.execute_reply.started": "2022-05-28T11:34:01.126223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_6(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    the total amount of communications verbs combined with the total amount of mental state verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    # Creates a list of communication verbs:\n",
    "    list_of_communication_verbs = [\"render\", \"imply\", \"talk\", \"affirm\", \"utter\", \"state\", \"declare\", \"voice\", \"express\", \"pronounce\", \"articulate\", \"enunciate\", \"voclize\", \"verbalize\", \"suggest\", \"indicate\", \"convey\", \"chat\", \"gossip\", \"prattle\", \"gab\", \"blather\", \"communicate\", \"negotiate\", \"parley\", \"confabulate\", \"prate\", \"confess\", \"mouth\", \"profess\", \"answer\", \"remark\", \"speak\", \"say\", \"tell\", \"whisper\", \"murmur\", \"mumble\", \"mutter\", \"yell\", \"shout\", \"cry\", \"call\", \"roar\", \"howl\", \"bellow\", \"bawl\", \"cheer\", \"yelp\", \"squawk\", \"shriek\", \"scream\", \"screech\", \"squeal\", \"squall\", \"whoop\", \"holler\", \"clamour\", \"caterwaul\", \"yawp\", \"vociferate\"]\n",
    "    # Creates a list of mental_state_verbs:\n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list or in the communication_verbs lsit: \n",
    "    Frecuency_dictionary_communication_and_mental_state_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_communication_verbs + list_of_mental_state_verbs)\n",
    "    #returns the frequency dictionary:\n",
    "    #Creates a variable with frequencies:\n",
    "    values = Frecuency_dictionary_communication_and_mental_state_verbs.values()\n",
    "    #Sum the values:\n",
    "    total = sum(values)\n",
    "    #Creates new dictionary:\n",
    "    Frecuency_dictionary_communication_and_mental_state_verbs = {\"communication and mental state verbs\": total} \n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_communication_and_mental_state_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd892933-12a6-45ff-a222-1871f64c1e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:07.029764Z",
     "iopub.status.busy": "2022-05-28T11:34:07.029109Z",
     "iopub.status.idle": "2022-05-28T11:34:18.400684Z",
     "shell.execute_reply": "2022-05-28T11:34:18.399621Z",
     "shell.execute_reply.started": "2022-05-28T11:34:07.029709Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_6(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_6 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5384e4e5-faae-4809-9c0d-9391512b615f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:20.155510Z",
     "iopub.status.busy": "2022-05-28T11:34:20.154978Z",
     "iopub.status.idle": "2022-05-28T11:34:20.164849Z",
     "shell.execute_reply": "2022-05-28T11:34:20.164127Z",
     "shell.execute_reply.started": "2022-05-28T11:34:20.155455Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4666666666666667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_6, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70ed8402-be54-4eee-90f2-6d279a0583c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:25.086239Z",
     "iopub.status.busy": "2022-05-28T11:34:25.085709Z",
     "iopub.status.idle": "2022-05-28T11:34:25.103917Z",
     "shell.execute_reply": "2022-05-28T11:34:25.103181Z",
     "shell.execute_reply.started": "2022-05-28T11:34:25.086186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mentalization_features_7(text_input):\n",
    "    \"\"\" \n",
    "    A function to subtract features from a .txt-file \n",
    "    to create a frequency dictionary based on specific words in this case:\n",
    "    the total amount of communications verbs combined with the total amount of mental state verbs\n",
    "    \n",
    "    Uses: \n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('universal_tagset')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk import pos_tag\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization of the text:\n",
    "    # Creates a variable with WordNet's lemmatizer function:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizes the text using the tokenizer function from above:\n",
    "    text = tokenizer_function(text_input)\n",
    "    #pos-tags the tokens where the pos-tags are set as universal:\n",
    "    words_with_pos = nltk.pos_tag(text, tagset = \"universal\")\n",
    "    #Creates a list with all the words with the tag: VERB\n",
    "    verbs_with_pos = [word for word in words_with_pos if word[1]=='VERB']\n",
    "    #takes first element of the tuples and saves them in a variable to remove the VERB tag:\n",
    "    verbs = [tuples[0] for tuples in verbs_with_pos]    \n",
    "    #Creates an empty list: \n",
    "    lemmatized_verbs = []\n",
    "    # lemmatizes all words based on the information that they are verbs: \"v\" according to WordNet's system and appends them to empty list:\n",
    "    for verb in verbs:\n",
    "        lemmatized_verb = lemmatizer.lemmatize(verb, pos = \"v\")\n",
    "        lemmatized_verbs.append(lemmatized_verb)\n",
    "    # Creates a list of communication verbs:\n",
    "    list_of_communication_verbs = [\"render\", \"imply\", \"talk\", \"affirm\", \"utter\", \"state\", \"declare\", \"voice\", \"express\", \"pronounce\", \"articulate\", \"enunciate\", \"voclize\", \"verbalize\", \"suggest\", \"indicate\", \"convey\", \"chat\", \"gossip\", \"prattle\", \"gab\", \"blather\", \"communicate\", \"negotiate\", \"parley\", \"confabulate\", \"prate\", \"confess\", \"mouth\", \"profess\", \"answer\", \"remark\", \"speak\", \"say\", \"tell\", \"whisper\", \"murmur\", \"mumble\", \"mutter\", \"yell\", \"shout\", \"cry\", \"call\", \"roar\", \"howl\", \"bellow\", \"bawl\", \"cheer\", \"yelp\", \"squawk\", \"shriek\", \"scream\", \"screech\", \"squeal\", \"squall\", \"whoop\", \"holler\", \"clamour\", \"caterwaul\", \"yawp\", \"vociferate\"]\n",
    "    # Creates a list of mental_state_verbs:\n",
    "    list_of_mental_state_verbs = [\"suppose\", \"fantasize\", \"pretend\", \"muse\", \"daydream\", \"hanker\", \"presuppose\", \"surmise\", \"conjecture\", \"reckon\", \"opine\", \"project\", \"scheme\",  \"recall\", \"recollect\", \"place\", \"picture\", \"envision\", \"visualize\", \"envisage\", \"plan\", \"conceptualize\", \"assume\", \"presume\", \"desiderate\", \"like\", \"yearn\", \"know\", \"desire\", \"crave\", \"aspire\", \"pine\", \"long\", \"dream\", \"seek\", \"lust\", \"realise\", \"register\", \"discern\", \"grasp\", \"comprehend\", \"apprehend\", \"conceive\", \"ascertain\", \"find\", \"get\", \"figure\", \"cognize\", \"deduce\", \"conclude\", \"see\", \"sense\", \"think\", \"learn\", \"understand\", \"perceive\", \"feel\", \"guess\", \"recognize\", \"notice\", \"want\", \"wish\", \"hope\", \"decide\", \"expect\", \"prefer\", \"remember\", \"forget\", \"imagine\", \"believe\"]\n",
    "    #Creates a frequency dictionary of the words if the word occurs in the mental state list or in the communication_verbs lsit: \n",
    "    Frecuency_dictionary_communication_and_mental_state_verbs = FreqDist(word for word in lemmatized_verbs if word in list_of_communication_verbs + list_of_mental_state_verbs)\n",
    "     #Goes trough the keys and values of the dictionary to make the keys into the words and the values into the frequency in percentage.\n",
    "    for (word, frequency) in Frecuency_dictionary_communication_and_mental_state_verbs.items():\n",
    "        Frecuency_dictionary_communication_and_mental_state_verbs[word] = frequency/len(verbs) \n",
    "    #Creates a variable with frequencies:\n",
    "    values = Frecuency_dictionary_communication_and_mental_state_verbs.values()\n",
    "    #Sum the values:\n",
    "    total = sum(values)\n",
    "    #Creates new dictionary:\n",
    "    Frecuency_dictionary_communication_and_mental_state_verbs = {\"communication and mental state verbs\": total} \n",
    "    #returns the frequency dictionary:\n",
    "    return Frecuency_dictionary_communication_and_mental_state_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb28c0c7-0973-4eb2-b693-2af66d96c366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:26.454353Z",
     "iopub.status.busy": "2022-05-28T11:34:26.453831Z",
     "iopub.status.idle": "2022-05-28T11:34:37.344219Z",
     "shell.execute_reply": "2022-05-28T11:34:37.343589Z",
     "shell.execute_reply.started": "2022-05-28T11:34:26.454300Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saves the state of the shuffled list so the same testing and training set is used for the classifiers\n",
    "random.seed(1)\n",
    "#Shuffles the list of files and their tags:\n",
    "random.shuffle(text_corpora_with_tags)\n",
    "# Defines featuesets as a list of tuples where the first element is the output from the mentalization_features function, and second element is the (TRUE/FALSE)-tag\n",
    "featuresets = [(mentalization_features_7(text), tag) for (text,tag) in text_corpora_with_tags]\n",
    "#Divides the data into a training-set of the first 30 elements and the testing-set as every after the first 30 elements.\n",
    "train_set, test_set = featuresets[30:], featuresets[:30]\n",
    "#Uses Naive Bayes classifier to train on the train set:\n",
    "classifier_7 = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bcfdcc2-a46c-405a-b635-e5e626d55fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T11:34:38.182901Z",
     "iopub.status.busy": "2022-05-28T11:34:38.182407Z",
     "iopub.status.idle": "2022-05-28T11:34:38.192542Z",
     "shell.execute_reply": "2022-05-28T11:34:38.191573Z",
     "shell.execute_reply.started": "2022-05-28T11:34:38.182848Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the accuracy based on the test set:\n",
    "nltk.classify.accuracy(classifier_6, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
